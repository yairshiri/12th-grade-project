agent:
  hyperparameters:
    alpha: 0.005
    gamma: 0.9
  mode: training
  model:
    copy interval: 1000
    layer sizes:
    - 64
    - 64
    learn interval: 4
  number of actions: 8
  policy:
    epsilon:
      decay: 0.99999
      min: 0.1
      starting: 1.0
    type: epsgreedy
  replay buffer:
    batch size: 128
    max size: 100000
    min size: 1000
    type: per
  train until:
    max episodes: 0
    max steps: 0
    max win rate: 0
    max win rate over x: 100.0
    to meet: 1
  type: DQN
  use sensors: false
environment:
  max steps per episode: 700
  starting player pos:
    x: 25
    y: 23
game:
  fps cap: 0
  maze shape:
    height: 30
    width: 30
  screen size:
    height: 460
    width: 680
logging:
  graph interval: 100
  metrics:
    avg episode reward: false
    fps: true
    number of episodes: true
    number of steps: true
    number of wins: true
    win rate: true
    win rate over x: true
paths:
  algorithm name: player algorithm
  maze name: weird 30x30
