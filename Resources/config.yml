agent:
  hyperparameters:
    alpha: 0.005
    gamma: 0.9
  mode: training
  model:
    copy interval: 1000
    layer sizes:
    - 512
    - 100
    learn interval: 4
  number of actions: 9
  policy:
    epsilon:
      decay: 0.99999
      min: 0.1
      starting: 1.0
    type: bepsgreedy
  replay buffer:
    batch size: 32
    max size: 100000
    min size: 1000
    type: per
  train until:
    max episodes: 0
    max steps: 0
    max win rate: 0
    max win rate over x: 100.0
    to meet: 1
  type: DQN
  use sensors: false
environment:
  max steps per episode: 500
  starting player pos:
    x: 9
    y: 9
  use conv: false
game:
  fps cap: 0
  maze shape:
    height: 20
    width: 20
  screen size:
    height: 460
    width: 680
logging:
  graph interval: 100
  metrics:
    avg episode reward: false
    fps: true
    number of episodes: true
    number of steps: true
    number of wins: true
    win rate: true
    win rate over x: true
paths:
  algorithm name: player algorithm
  maze name: empty 20x20
